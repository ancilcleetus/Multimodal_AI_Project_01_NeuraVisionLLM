{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Evolution of Large Language Models: RNNs to Transformers and Beyond"
      ],
      "metadata": {
        "id": "F6dfM34sf_l1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "D0XaQAZ-g549"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“– TABLE OF CONTENTS\n",
        "\n",
        "- [Section 1]()\n",
        "  - [Subsection 1]()\n",
        "    - [Subsubsection 1]()\n",
        "    - [Subsubsection 2]()\n",
        "  - [Subsection 2]()\n",
        "    - [Subsubsection 1]()\n",
        "    - [Subsubsection 2]()\n",
        "- [Section 2]()\n",
        "  - [Subsection 1]()\n",
        "    - [Subsubsection 1]()\n",
        "    - [Subsubsection 2]()\n",
        "  - [Subsection 2]()\n",
        "    - [Subsubsection 1]()\n",
        "    - [Subsubsection 2]()"
      ],
      "metadata": {
        "id": "SIhfEEeuxaVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "uLR90WhedwER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction"
      ],
      "metadata": {
        "id": "bKim3KwyTp11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The field of Natural Language Processing (NLP) has witnessed groundbreaking advancements over the past few decades, with the evolution of Large Language Models (LLMs) at its core. These models have revolutionized how machines understand, generate, and interact with human language, paving the way for transformative applications across industries.\n",
        "\n",
        "The journey began with Recurrent Neural Networks (RNNs), a class of models designed to process sequential data by maintaining contextual information through hidden states. However, RNNs faced challenges like vanishing gradients and limited long-term memory, which constrained their ability to model complex language patterns effectively.\n",
        "\n",
        "To address these limitations, researchers introduced architectural innovations such as Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs). These enhanced versions of RNNs improved performance in handling sequential data but still struggled with scalability and efficiency.\n",
        "\n",
        "The advent of the Transformer architecture marked a paradigm shift in NLP. By leveraging self-attention mechanisms and parallel processing, Transformers overcame the limitations of sequential processing inherent in RNN-based models. This innovation not only improved performance but also unlocked the potential for training models on massive datasets, leading to the development of Large Language Models like BERT, GPT, and beyond.\n",
        "\n",
        "This notebook explores the evolution of these models, delving into their architecture, capabilities, and the pivotal role they play in shaping modern AI. We will chart the progression from RNNs to Transformers, uncovering how these advancements have set the stage for cutting-edge applications in NLP and multimodal AI."
      ],
      "metadata": {
        "id": "AxhkXGXDuFUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "U897-NGnYhHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difficulty: ${\\color{green}{Easy}}$\n",
        "Difficulty: ${\\color{orange}{Medium}}$\n",
        "Difficulty: ${\\color{red}{Hard}}$"
      ],
      "metadata": {
        "id": "Pf0qrlJcsUj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Learning as subset of ML\n",
        "\n",
        "from IPython import display\n",
        "display.Image(\"data/images/01-Deep-Learning-Foundations/CampusX-Deep-Learning-Course/DL_01_Intro-01-DL-subset-of-ML.jpg\")"
      ],
      "metadata": {
        "id": "juNAxVBFg7sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "qqv0L0R9dyKJ"
      }
    }
  ]
}